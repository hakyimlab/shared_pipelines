{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing running pytorch models using the common pipeline functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 16:42:26.882972: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using this config file: /grand/gpu_hack/imlab/users/saideep/test_inference/config.json\n"
     ]
    }
   ],
   "source": [
    "import os, sys, json\n",
    "import pandas as pd # for manipulating dataframes\n",
    "import time\n",
    "from datetime import date\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "\n",
    "batch_utils_path = \"/grand/gpu_hack/imlab/users/saideep/shared_folder/enformer_pipeline/scripts/modules\"\n",
    "sys.path.append(batch_utils_path)\n",
    "\n",
    "import predictionUtils\n",
    "import predictUtils_one\n",
    "import predictUtils_two\n",
    "import sequencesUtils\n",
    "import enformer_pytorch\n",
    "import attention\n",
    "import loggerUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May  4 16:42:30 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.103.04   Driver Version: 470.103.04   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    52W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM...  On   | 00000000:46:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    53W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM...  On   | 00000000:85:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    52W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM...  On   | 00000000:C7:00.0 Off |                    0 |\n",
      "| N/A   33C    P0    54W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start by building a test sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_file = \"/grand/TFXcan/imlab/data/hg_sequences/hg38/Homo_sapiens_assembly38.fasta\"\n",
    "\n",
    "test_region = \"chr5_2000000_2000100\"\n",
    "seq_length_basenji = 131072\n",
    "fasta_extractor = fasta_extractor = sequencesUtils.get_fastaExtractor(fasta_file)\n",
    "\n",
    "test_seq = sequencesUtils.extract_reference_sequence(test_region, fasta_extractor, resize_for_enformer=False, resize_length=seq_length_basenji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': array([[[0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.]]], dtype=float32), 'interval_object': {'chr': 'chr5', 'start': 1934515, 'end': 2065587}}\n",
      "(1, 131072, 4)\n"
     ]
    }
   ],
   "source": [
    "print(test_seq)\n",
    "print(test_seq[\"sequence\"].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we should load a test pytorch model now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Enformer(\n",
       "  (_trunk): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Rearrange('b l c -> b c l')\n",
       "      (1): Conv1d(4, 768, kernel_size=(15,), stride=(1,), padding=same)\n",
       "      (2): Residual(\n",
       "        (_module): Sequential(\n",
       "          (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(768, 768, kernel_size=(1,), stride=(1,), padding=same)\n",
       "        )\n",
       "      )\n",
       "      (3): SoftmaxPooling1D(\n",
       "        (_logit_linear): Linear(in_features=768, out_features=768, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(768, 768, kernel_size=(5,), stride=(1,), padding=same)\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (_module): Sequential(\n",
       "            (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(768, 768, kernel_size=(1,), stride=(1,), padding=same)\n",
       "          )\n",
       "        )\n",
       "        (2): SoftmaxPooling1D(\n",
       "          (_logit_linear): Linear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(768, 896, kernel_size=(5,), stride=(1,), padding=same)\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (_module): Sequential(\n",
       "            (0): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(896, 896, kernel_size=(1,), stride=(1,), padding=same)\n",
       "          )\n",
       "        )\n",
       "        (2): SoftmaxPooling1D(\n",
       "          (_logit_linear): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(896, 1024, kernel_size=(5,), stride=(1,), padding=same)\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (_module): Sequential(\n",
       "            (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,), padding=same)\n",
       "          )\n",
       "        )\n",
       "        (2): SoftmaxPooling1D(\n",
       "          (_logit_linear): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(1024, 1152, kernel_size=(5,), stride=(1,), padding=same)\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (_module): Sequential(\n",
       "            (0): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(1152, 1152, kernel_size=(1,), stride=(1,), padding=same)\n",
       "          )\n",
       "        )\n",
       "        (2): SoftmaxPooling1D(\n",
       "          (_logit_linear): Linear(in_features=1152, out_features=1152, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(1152, 1280, kernel_size=(5,), stride=(1,), padding=same)\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (_module): Sequential(\n",
       "            (0): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(1280, 1280, kernel_size=(1,), stride=(1,), padding=same)\n",
       "          )\n",
       "        )\n",
       "        (2): SoftmaxPooling1D(\n",
       "          (_logit_linear): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(1280, 1536, kernel_size=(5,), stride=(1,), padding=same)\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (_module): Sequential(\n",
       "            (0): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(1536, 1536, kernel_size=(1,), stride=(1,), padding=same)\n",
       "          )\n",
       "        )\n",
       "        (2): SoftmaxPooling1D(\n",
       "          (_logit_linear): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Rearrange('b c l -> b l c')\n",
       "      (1): Sequential(\n",
       "        (0): Residual(\n",
       "          (_module): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): MultiHeadAttention(\n",
       "              (_q_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (_k_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (_v_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (_embedding_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (_rel_pos_layer): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (_pos_dropout_layer): Dropout(p=0.01, inplace=False)\n",
       "              (_attn_dropout_layer): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (_module): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Residual(\n",
       "          (_module): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): MultiHeadAttention(\n",
       "              (_q_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (_k_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (_v_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (_embedding_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (_rel_pos_layer): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (_pos_dropout_layer): Dropout(p=0.01, inplace=False)\n",
       "              (_attn_dropout_layer): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (_module): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Residual(\n",
       "          (_module): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): MultiHeadAttention(\n",
       "              (_q_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (_k_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (_v_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (_embedding_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (_rel_pos_layer): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (_pos_dropout_layer): Dropout(p=0.01, inplace=False)\n",
       "              (_attn_dropout_layer): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (_module): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): Residual(\n",
       "          (_module): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): MultiHeadAttention(\n",
       "              (_q_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (_k_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (_v_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (_embedding_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (_rel_pos_layer): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (_pos_dropout_layer): Dropout(p=0.01, inplace=False)\n",
       "              (_attn_dropout_layer): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (_module): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): Residual(\n",
       "          (_module): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): MultiHeadAttention(\n",
       "              (_q_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (_k_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (_v_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (_embedding_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (_rel_pos_layer): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (_pos_dropout_layer): Dropout(p=0.01, inplace=False)\n",
       "              (_attn_dropout_layer): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (_module): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): Residual(\n",
       "          (_module): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): MultiHeadAttention(\n",
       "              (_q_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (_k_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (_v_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (_embedding_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (_rel_pos_layer): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (_pos_dropout_layer): Dropout(p=0.01, inplace=False)\n",
       "              (_attn_dropout_layer): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (_module): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): Residual(\n",
       "          (_module): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): MultiHeadAttention(\n",
       "              (_q_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (_k_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (_v_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (_embedding_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (_rel_pos_layer): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (_pos_dropout_layer): Dropout(p=0.01, inplace=False)\n",
       "              (_attn_dropout_layer): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (_module): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): Sequential(\n",
       "        (0): Residual(\n",
       "          (_module): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): MultiHeadAttention(\n",
       "              (_q_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (_k_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (_v_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (_embedding_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (_rel_pos_layer): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (_pos_dropout_layer): Dropout(p=0.01, inplace=False)\n",
       "              (_attn_dropout_layer): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (_module): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): Sequential(\n",
       "        (0): Residual(\n",
       "          (_module): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): MultiHeadAttention(\n",
       "              (_q_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (_k_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (_v_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (_embedding_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (_rel_pos_layer): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (_pos_dropout_layer): Dropout(p=0.01, inplace=False)\n",
       "              (_attn_dropout_layer): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (_module): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): Sequential(\n",
       "        (0): Residual(\n",
       "          (_module): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): MultiHeadAttention(\n",
       "              (_q_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (_k_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (_v_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (_embedding_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (_rel_pos_layer): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (_pos_dropout_layer): Dropout(p=0.01, inplace=False)\n",
       "              (_attn_dropout_layer): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (_module): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): Sequential(\n",
       "        (0): Residual(\n",
       "          (_module): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): MultiHeadAttention(\n",
       "              (_q_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (_k_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (_v_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (_embedding_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (_rel_pos_layer): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (_pos_dropout_layer): Dropout(p=0.01, inplace=False)\n",
       "              (_attn_dropout_layer): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (_module): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): TargetLengthCrop()\n",
       "    (4): Sequential(\n",
       "      (0): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "      (1): Dropout(p=0.05, inplace=False)\n",
       "      (2): GELU()\n",
       "    )\n",
       "  )\n",
       "  (_heads): ModuleDict(\n",
       "    (human): Sequential(\n",
       "      (0): Linear(in_features=3072, out_features=5313, bias=True)\n",
       "      (1): Softplus(beta=1, threshold=20)\n",
       "    )\n",
       "    (mouse): Sequential(\n",
       "      (0): Linear(in_features=3072, out_features=1643, bias=True)\n",
       "      (1): Softplus(beta=1, threshold=20)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = enformer_pytorch.Enformer().to(\"cuda\")\n",
    "print(torch.cuda.is_available())\n",
    "model.load_state_dict(torch.load(\"/grand/gpu_hack/imlab/users/temi/projects/alcf-hackathon-2023/outputs/outputs-2023-05-04-060420/model_1500.pt\"))\n",
    "model.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has been loaded, we can run the test inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_out = model(torch.tensor(test_seq[\"sequence\"]).to(\"cuda\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produces a dictionary of tensors. We want to extract just the human output and then remove the object from memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "human_out_cpu = test_out['human'].cpu().detach().numpy()\n",
    "\n",
    "del test_out\n",
    "torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a copy of the human output into cpu (host) memory, and then we clear gpu space by deleting the original outputs.\n",
    "Now we can save the outputs as an hdf5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chr5_2000000_2000100', 'test', 'completed', 'ref']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_out = model(torch.tensor(test_seq[\"sequence\"]).to(\"cuda\"))\n",
    "\n",
    "test_out_dir = \"/grand/gpu_hack/imlab/users/saideep/test_inference\"\n",
    "\n",
    "sample_predictions = {\n",
    "    \"haplo1\": human_out_cpu\n",
    "}\n",
    "\n",
    "test_meta = {\n",
    "    \"region\" : test_region,\n",
    "    \"sequence_source\" : \"ref\"\n",
    "}\n",
    "\n",
    "loggerUtils.save_haplotypes_h5_prediction(haplotype_predictions=sample_predictions, \n",
    "                                          metadata=test_meta, \n",
    "                                          output_dir=test_out_dir, sample=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_out_path = \"/grand/gpu_hack/imlab/users/saideep/test_inference/test/haplo1/chr5_2000000_2000100_predictions.h5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "chr5_2000000_2000100\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 28\u001b[0m\n\u001b[1;32m     24\u001b[0m cur_out \u001b[39m=\u001b[39m model(torch\u001b[39m.\u001b[39mtensor(cur_seq[\u001b[39m\"\u001b[39m\u001b[39msequence\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m     26\u001b[0m human_out_cpu \u001b[39m=\u001b[39m cur_out[\u001b[39m'\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m---> 28\u001b[0m \u001b[39mdel\u001b[39;00m test_out\n\u001b[1;32m     29\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n\u001b[1;32m     32\u001b[0m sample_predictions \u001b[39m=\u001b[39m {\n\u001b[1;32m     33\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mref\u001b[39m\u001b[39m\"\u001b[39m: human_out_cpu\n\u001b[1;32m     34\u001b[0m }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_out' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# FULL LOOP\n",
    "\n",
    "fasta_file = \"/grand/TFXcan/imlab/data/hg_sequences/hg38/Homo_sapiens_assembly38.fasta\"\n",
    "\n",
    "seq_length_basenji = 131072\n",
    "fasta_extractor = fasta_extractor = sequencesUtils.get_fastaExtractor(fasta_file)\n",
    "\n",
    "model = enformer_pytorch.Enformer().to(\"cuda\")\n",
    "print(torch.cuda.is_available())\n",
    "model.load_state_dict(torch.load(\"/grand/gpu_hack/imlab/users/temi/projects/alcf-hackathon-2023/outputs/outputs-2023-05-04-060420/model_1500.pt\"))\n",
    "model.eval()\n",
    "\n",
    "out_dir = \"/grand/gpu_hack/imlab/users/saideep/test_inference\"\n",
    "\n",
    "with open(\"/grand/gpu_hack/imlab/data/basenji_data_pt/human/sequences.bed\", \"r\") as seqs:\n",
    "    for line in seqs:\n",
    "        if line.strip().split(\"\\t\")[-1] != \"test\":\n",
    "            continue\n",
    "        cur_region = \"_\".join(line.strip().split(\"\\t\")[0:3])\n",
    "        print(test_region)\n",
    "\n",
    "        cur_seq = sequencesUtils.extract_reference_sequence(cur_region, fasta_extractor, resize_for_enformer=False, resize_length=seq_length_basenji)\n",
    "\n",
    "        cur_out = model(torch.tensor(cur_seq[\"sequence\"]).to(\"cuda\"))\n",
    "        \n",
    "        human_out_cpu = cur_out['human'].cpu().detach().numpy()\n",
    "\n",
    "        del cur_out\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "\n",
    "        sample_predictions = {\n",
    "            \"ref\": human_out_cpu\n",
    "        }\n",
    "\n",
    "        test_meta = {\n",
    "            \"region\" : cur_region,\n",
    "            \"sequence_source\" : \"ref\"\n",
    "        }\n",
    "\n",
    "        loggerUtils.save_haplotypes_h5_prediction(haplotype_predictions=sample_predictions, \n",
    "                                                metadata=test_meta, \n",
    "                                                output_dir=out_dir, sample=\"test\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42505273344 19547553792 9014983168\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42505273344 19547553792 9015886848\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
